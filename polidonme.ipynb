{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   1.]\n",
      " [ -1.  nan   0.  -1.]\n",
      " [ -1.  -1.  -1.  -1.]]\n",
      "\n",
      "[[  1.   1.   1.   0.]\n",
      " [  0.  nan   1.   0.]\n",
      " [  0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#############\n",
    "# MDP CLASS #\n",
    "#############\n",
    "class Holder:\n",
    "    \"\"\"\n",
    "    Create Marcov Decision Processes\n",
    "    Input:\n",
    "        State = array with Gridworlds (Strings)\n",
    "        Actions = predefined possible actions (up, down, left, right)\n",
    "        probability = probability of intended state change\n",
    "        reward = standard short-term reward\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, State, probability, reward, zip_policy, discount_factor):\n",
    "        \"\"\"\n",
    "        call to get Gridworld\n",
    "        \"\"\" \n",
    "        self.field = State\n",
    "        self.zip_policy = zip_policy\n",
    "        self.prob = probability\n",
    "        self.reward = reward\n",
    "        self.discount_factor = discount_factor\n",
    "     \n",
    "     \n",
    "        \n",
    "##################\n",
    "# # EVALUATION # #\n",
    "##################\n",
    "\n",
    "def old_value(field,new_state,old,state):\n",
    "    \"\"\"\n",
    "    return value of new state\n",
    "    if the new state is an obsticle we do not move\n",
    "    and return the value of the current state\n",
    "    \"\"\"\n",
    "    \n",
    "    i = (int)(new_state[0])\n",
    "    j = (int)(new_state[1])\n",
    "    \n",
    "    old_i = (int)(state[0])\n",
    "    old_j = (int)(state[1])\n",
    "    \n",
    "    M,N = field.shape\n",
    "    \n",
    "    # Not performing an action if we try to leave the grid world or move against an obstacle\n",
    "    # => no state transition\n",
    "    if i < 0 or i >= M or j < 0 or j >= N or field[i][j] == 'O':\n",
    "        return old[old_i,old_j]\n",
    "   \n",
    "    return old[i,j]\n",
    "\n",
    "def v(state, old, holder):\n",
    "    \"\"\"\n",
    "    calculate value of state following policy\n",
    "    \n",
    "    state as current state (tuple)\n",
    "    mdp as MDP object which holds field, probability, reward\n",
    "    discount_factor as float\n",
    "    policy as 2d array\n",
    "    \"\"\"\n",
    "    field = holder.field\n",
    "    \n",
    "    # retrieve action (movement in x and y direction) from policy\n",
    "    x_policy, y_policy = holder.zip_policy\n",
    "    x,y = x_policy[state],y_policy[state]\n",
    "    \n",
    "    # calculate new state\n",
    "    # add x,y for intended new state\n",
    "    # add y,x for moving to the right of intended\n",
    "    # add -y,-x for moving to the left of intended\n",
    "    state_1 = (state[0] + x, state[1] + y)\n",
    "    state_2 = (state[0] + y, state[1] + x)\n",
    "    state_3 = (state[0] - y, state[1] - x)\n",
    "    \n",
    "    # probability of moving in an unintended direction\n",
    "    prob = holder.prob\n",
    "    un_prob = (1 - holder.prob/2)\n",
    "    \n",
    "    # formula from slides with old value function\n",
    "    \n",
    "    return (holder.reward + holder.discount_factor * (prob * old_value(field,state_1,old,state)\n",
    "                                                + un_prob * old_value(field,state_2,old,state) \n",
    "                                                + un_prob * old_value(field,state_3,old,state)))\n",
    "\n",
    "def evaluation(holder,iterations):\n",
    "    \"\"\"\n",
    "    policy evaluation\n",
    "    \n",
    "    in:\n",
    "    field original grid world\n",
    "    probability as float\n",
    "    reward as float\n",
    "    policy as dictionary (state -> action)\n",
    "    discount_factor as float\n",
    "    \n",
    "    return evaluated policy as value function\n",
    "    \"\"\"\n",
    "    \n",
    "    # get original grid world\n",
    "    field = holder.field\n",
    "    M,N = field.shape\n",
    "    \n",
    "    # create a 2d array which is going to hold the previous value matrix for comparison\n",
    "    old = np.zeros((M,N))\n",
    "    v_matrix = np.zeros((M,N))\n",
    "    \n",
    "    # evaluate policy n times\n",
    "    for _ in range(iterations):\n",
    "        \n",
    "        # new value matrix is all zeros\n",
    "        v_matrix = np.zeros((M,N))\n",
    "        \n",
    "        # iterate over each and every state and perform updates\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                \n",
    "                # if there is an 'O' in the grid world we do not want to take this field into account\n",
    "                # therefore we will assign -99 to the field (None's are bad for comparison)\n",
    "                # we also ignore 'O' fields when doing the greedy policy update\n",
    "                # => no need to worry about this artificial negative 99\n",
    "                if field[i,j] == 'O':\n",
    "                    v_matrix[i,j] = None\n",
    "                    old[i,j] = None\n",
    "                    \n",
    "                # if state is exit, value = 1\n",
    "                if field[i,j] == 'E':\n",
    "                    v_matrix[i,j] = 1\n",
    "                    old[i,j] = 1\n",
    "                \n",
    "                # if state is pitfall, value = -1 \n",
    "                if field[i,j] == 'P':\n",
    "                    v_matrix[i,j] = -1\n",
    "                    old[i,j] = -1\n",
    "                    \n",
    "                # if state is normal field, calculate new value\n",
    "                if field[i,j] == 'F':\n",
    "                    v_matrix[i,j] = v((i,j),old,holder)\n",
    "                            \n",
    "        # calculated value matrix is now old matrix\n",
    "        old = np.copy(v_matrix)\n",
    "    \n",
    "    return v_matrix\n",
    " \n",
    "########################\n",
    "# # POLICY ITERATION # #\n",
    "########################\n",
    "def _iterate(v, holder, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over every State once\n",
    "    \n",
    "    Input: \n",
    "        v = value function 2D-Array of expectred reward at each state\n",
    "        MDP = MDP-Object (Markov-Decision-Processes) for which the optimal policy should be found\n",
    "        policy = policy 2D-Array elem = action/ direction to move\n",
    "        discount_factor = for policy evalutatin float elem[0,1]\n",
    "    Return:\n",
    "        v , policy\n",
    "    \"\"\"\n",
    "    value_function = evaluation(holder, iterations)\n",
    "    x_policy, y_policy = holder.zip_policy\n",
    "    M, N = holder.field.shape\n",
    "    \n",
    "    # obstacle padding\n",
    "    biggerState =[]\n",
    "    \n",
    "    for i in range(M+2):\n",
    "        sublist = []\n",
    "        for j in range(N+2):\n",
    "            sublist.append('O')\n",
    "        biggerState.append(sublist)\n",
    "     \n",
    "    biggerState = np.asarray(biggerState)\n",
    "    \n",
    "    biggerState[1:M+1,1:N+1] = holder.field\n",
    "    \n",
    "    bigger = np.ones((M+2,N+2)) * -9999999999\n",
    "    bigger[1:M+1,1:N+1] = value_function\n",
    "    value_function = bigger\n",
    "    \n",
    "    for i,j in np.argwhere(biggerState != 'O'):\n",
    "            \n",
    "            # finding max in 4 nbh\n",
    "            # lets improve this part pls\n",
    "            \n",
    "            elem_list = []\n",
    "            elem_list.append(value_function[i-1,j])\n",
    "            elem_list.append(value_function[i+1,j])\n",
    "            elem_list.append(value_function[i,j+1])\n",
    "            elem_list.append(value_function[i,j-1])\n",
    "            \n",
    "            cords_list = []\n",
    "            cords_list.append((i-1,j))\n",
    "            cords_list.append((i+1,j))\n",
    "            cords_list.append((i,j+1))\n",
    "            cords_list.append((i,j-1))\n",
    "            \n",
    "            max_point =  cords_list[elem_list.index(max(elem_list))]\n",
    "                  \n",
    "            x,y = tuple(np.subtract(max_point,(i,j)))\n",
    "            \n",
    "            x_policy[i-1,j-1] = x\n",
    "            y_policy[i-1,j-1] = y\n",
    "    \n",
    "    policy = (x_policy,y_policy)\n",
    "    \n",
    "    return value_function[1:M+1,1:N+1],policy\n",
    "\n",
    "def _policyIteration(holder, iterations = 50):\n",
    "    \"\"\"\n",
    "    Find optimal policy by iterating over the policy until stopping-condition \n",
    "    is met either until converges or amount of steps reached\n",
    "\n",
    "    Input:\n",
    "        MDP = MDP-Object (Markov-Decision-Processes) for which the optimal policy should be found\n",
    "        discount_factor = for policy evalutatin float elem[0,1]\n",
    "        iterations = number of calls, not stated do until policy converges\n",
    "\n",
    "    Return:\n",
    "        Optimal policy as 2D-array\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize value function\n",
    "    M, N = holder.field.shape\n",
    "    v_function = np.zeros((M,N))\n",
    "    \n",
    "    if callable(iterations):\n",
    "        #iterate over policy n-times\n",
    "        for i in range(iterations):\n",
    "            v_function, zip_policy = _iterate(v_function, holder,iterations)\n",
    "    \n",
    "    else:\n",
    "        #iterate over policy until converges\n",
    "        \n",
    "        while((v_function.any() != evaluation(holder,iterations).any())):\n",
    "            v_function, zip_policy = _iterate(v_function, holder,iterations)\n",
    "                \n",
    "    return zip_policy\n",
    " \n",
    "########\n",
    "# MAIN #\n",
    "########\n",
    "import inspect, os #Imports\n",
    "\n",
    "debug = True #Global Switch for Debug-Info\n",
    "\n",
    "#Determine working directory and gridpath\n",
    "scriptpath = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "gridsubpath = \"/Grids/3by4.grid\"\n",
    "gridpath = scriptpath + gridsubpath\n",
    "\n",
    "#Read Grid-File as 2-dimensional array 'grid' from 'gridpath'\n",
    "with open(gridpath) as gridfile:\n",
    "    grid = [line.split() for line in gridfile]\n",
    "\n",
    "        \n",
    "field = np.asarray(grid)\n",
    "M,N = field.shape\n",
    "[[x,y]] = np.argwhere(field == 'O')\n",
    "\n",
    "x_policy = np.ones((M,N))\n",
    "x_policy[x,y] = None\n",
    "y_policy = np.zeros((M,N))\n",
    "y_policy[x,y] = None\n",
    "zip_policy = (x_policy,y_policy)\n",
    "\n",
    "discount_factor = 0.7\n",
    "\n",
    "holder = Holder(field,0.8,-0.04,zip_policy,discount_factor)\n",
    "\n",
    "# hoch =  -1 0 \n",
    "# rechts = 0 1\n",
    "# links = 0 -1\n",
    "# down = 1 0\n",
    "\n",
    "perfect_policy = _policyIteration(holder)\n",
    "\n",
    "# steffis kosmetische update funktion für die policy\n",
    "\n",
    "print(perfect_policy[0])\n",
    "print()\n",
    "print(perfect_policy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
